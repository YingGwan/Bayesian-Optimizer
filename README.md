# Bayesian-Optimizer (BO)

**Python implementation of Bayesian Optimization (Can be easily extend to high dimensional case)**

**Bayesian optimization** is a [sequential design](https://en.wikipedia.org/wiki/Sequential_analysis) strategy for [global optimization](https://en.wikipedia.org/wiki/Global_optimization) of [black-box](https://en.wikipedia.org/wiki/Black_box) functions[[1\]](https://en.wikipedia.org/wiki/Bayesian_optimization#cite_note-1) that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions. 

**Bayesian optimization** is a kind of *Monte Carlo* method which uses sampling to solve numerical problems. 

*Advantage*:

- Smaller number of accessing black-box function (compared to random sampling)
- The surrogate model generated by BO is an approximate of original complicated model, which makes the global optimization become possible (Compared to gradient-based optimization including newton and quasi-newton method)

*Disdvantage*:

- Sometimes the surrogate model approximated is not accurate when AF is not suitably chosen

  (See ./1d-bay/function 3/)

- Kernel selection is a kinda art LMAO



***Code Results***

- 1D Case

  - Code: ./1d-bay/bary1d.py

  ![ezgif.com-gif-maker (1)](https://raw.githubusercontent.com/YingGwan/TyporaUploadImg/main/typora202204/18/115844-608510.gif)

  

- 2D Case

  - Code: ./2d-bay/bary2d.py

![ezgif.com-gif-maker](https://raw.githubusercontent.com/YingGwan/TyporaUploadImg/main/typora202204/18/113446-775466.gif)





- Basic Idea: 
  - Treat every sample as a variable following **Gaussian Distribution**. [Wiki](https://en.wikipedia.org/wiki/Normal_distribution)
  - Joint distribution of several Gaussian variables following N-dimension Gaussian Distribution, named **"Gaussian Process (GP)"**. [Wiki](https://en.wikipedia.org/wiki/Gaussian_process)
  - Given already known variable values, we derivate  **The Conditional Distribution (CD)** of GP along with whole space. The mean is the predicted value while the standard deviation describes uncertainty at that point.
  - Utilize **Acquisition Function (AF)** to guide the flexible sampling of the next variable
    - This is a key challenge of Bayesian Optimization cuz we need to minimize the AF to acquire the next variable pos.  [Paper]( https://proceedings.neurips.cc/paper/2018/file/498f2c21688f6451d9f5fd09d53edda7-Paper.pdf)
  - Add newly acquired variable and get the black-box value of this variable and add it to the dataset. Repeat these steps until reaching a threshold.

